---
title: "Assignment_2 Lab1"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

Assignment 2 Lab1 by Madhvi Jha
# Applied Exercise 2.4.8
This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are

Private: Public/private indicator</br>
Apps: Number of applications received</br>
Accept: Number of applicants accepted</br>
Enroll: Number of new students enrolled</br>
Top10perc: New students from top 10% of high school class</br>
Top25perc: New students from top 25% of high school class</br>
F.Undergrad: Number of full-time undergraduates</br>
P.Undergrad: Number of part-time undergraduates</br>
Outstate: Out-of-state tuition</br
Room.Board: Room and board costs</br>
Books: Estimated book costs</br>
Personal: Estimated personal spending</br>
PhD: Percent of faculty with Ph.D.'s</br>
Terminal: Percent of faculty with terminal degree</br>
S.F.Ratio: Student/faculty ratio</br>
perc.alumni: Percent of alumni who donate</br>
Expend: Instructional expenditure per student<br>
Grad.Rate: Graduation rate</br>

## 2.4.8.a
**Use the read.csv() function to read the data into R.Call the loaded data college. Make sure that you have the directory set to the correct location for the data.**
```{r}
# install.packages("ISLR")
library(ISLR)
library(MASS)
# college = read.csv("file name")
# Instead of running above command we are getting from package
data("College")
college = na.omit(College)
```
## 2.4.8.b
**Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don't really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:**

</br>**It is not needed since we are not loading from csv**
However, we could have run below code if done from csv
</br>
rownames(college) = college[, 1]</br>
fix(college)
</br>

```{r}
head(college)
```
## 2.4.8.c
### Part 2.4.8.c.i
**Use the summary() function to produce a numerical summary of the variables in the data set.**
```{r}
summary(college)
```
### 2.4.8.c.ii
**Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[, 1:10].**

```{r}
pairs(college[,1:10])
```

### 2.4.8.c.iii
**Use the plot() function to produce side-by-side boxplots of Outstate versus Private.**

```{r}
plot(college$Private, college$Outstate, xlab = "Private", ylab = "Out-of-state tuition (dollars)")
```
### 2.4.8.c.iv
**Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.**

```{r}
Elite = rep("No", nrow(college))
Elite[college$Top10per > 50] = "Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
```
**Use the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.**

```{r}
summary(college$Elite)
```

```{r}
plot(college$Elite, college$Outstate, xlab = "Elite", ylab = "Out-of-state tuition (dollars)")
```
### 2.4.8.c.v
**Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow = c(2, 2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.**
```{r}
par(mfrow = c(2, 2))
hist(college$Apps, xlab = "Number of applicants", main = "Histogram for all colleges")
hist(college$Apps[college$Private == "Yes"], xlab = "Number of applicants", main = "Histogram for private schools")
hist(college$Apps[college$Private == "No"], xlab = "Number of applicants", main = "Histogram for public schools")
hist(college$Apps[college$Elite == "Yes"], xlab = "Number of applicants", main = "Histogram for elite schools")
```
```{r}
par(mfrow = c(2, 2))
hist(college$Expend, xlab = "Instructional expenditure per student (dollars)", main = "Histogram for all colleges")
hist(college$Expend[college$Private == "Yes"], xlab = "Instructional expenditure per student (dollars)", main = "Histogram for private schools")
hist(college$Expend[college$Private == "No"], xlab = "Instructional expenditure per student (dollars)", main = "Histogram for public schools")
hist(college$Expend[college$Elite == "Yes"], xlab = "Instructional expenditure per student (dollars)", main = "Histogram for elite schools")
```
```{r}
par(mfrow = c(2, 2))
hist(college$S.F.Ratio, xlab = "Student-Faculty Ratio", main = "Histogram for all colleges")
hist(college$S.F.Ratio[college$Private == "Yes"], xlab = "Student-Faculty Ratio", main = "Histogram for private schools")
hist(college$S.F.Ratio[college$Private == "No"], xlab = "Student-Faculty Ratio", main = "Histogram for public schools")
hist(college$S.F.Ratio[college$Elite == "Yes"], xlab = "Student-Faculty Ratio", main = "Histogram for elite schools")
```
### 2.4.8.c.vi
**Continue exploring the data, and provide a brief summary of what you discover.**
```{r}
NonTuitionCosts = college$Room.Board + college$Books + college$Personal
college = data.frame(college, NonTuitionCosts)
par(mfrow = c(1, 2))
plot(college$Private, college$NonTuitionCosts, xlab = "Private", ylab = "Total non-tuition costs per year (dollars)")
plot(college$Elite, college$NonTuitionCosts, xlab = "Elite", ylab = "Total non-tuition costs per year (dollars)")
```
Based on the above box plots, it looks like that, aside from some outlier schools with very high costs, there isn't a wide gap for the median non-tution costs between private schools and public schools. The box plots do show, though, that there is a distinct difference in median non-tuition costs between elite and non-elite schools, with elite schools having higher costs.

```{r}
AcceptPerc = college$Accept / college$Apps * 100
college = data.frame(college, AcceptPerc)
par(mfrow = c(1, 2))
plot(college$Private, college$AcceptPerc, xlab = "Private", ylab = "Acceptance Rate")
plot(college$Elite, college$AcceptPerc, xlab = "Elite", ylab = "Acceptance Rate")
```
```{r}
summary(college$AcceptPerc[college$Private == "Yes"])
```
```{r}
summary(college$AcceptPerc[college$Private == "No"])
```
```{r}
summary(college$AcceptPerc[college$Elite == "Yes"])
```
```{r}
summary(college$AcceptPerc[college$Elite == "No"])
```
The boxplots show that while the median acceptance rates for both private and public schools are pretty close at around 75-80%, private schools have a much wider range of acceptance rates (going down to a minimum of 15.45%). When we distinguish between elite and non-elite schools, elite schools have a much lower median acceptance rate compared to non-elite ones.
```{r}
par(mfrow = c(2, 2))
hist(college$perc.alumni, xlab = "Percent of alumni who donate", main = "Histogram for all colleges")
hist(college$perc.alumni[college$Private == "Yes"], xlab = "Percent of alumni who donate", main = "Histogram for private schools")
hist(college$perc.alumni[college$Private == "No"], xlab = "Percent of alumni who donate", main = "Histogram for public schools")
hist(college$perc.alumni[college$Elite == "Yes"], xlab = "Percent of alumni who donate", main = "Histogram for elite schools")
```
Based on the above histograms, private schools and elite schools tend to have a higher percent of alumni who donate.
```{r}
par(mfrow = c(2, 2))
plot(college$PhD, college$Grad.Rate, xlab = "Number of faculty with PhDs", ylab = "Graduation Rate")
plot(college$Terminal, college$Grad.Rate, xlab = "Number of faculty with terminal degrees", ylab = "Graduation Rate")
plot(college$S.F.Ratio, college$Grad.Rate, xlab = "Student-faculty ratio", ylab = "Graduation Rate")
plot(college$Expend, college$Grad.Rate, xlab = "Instructional expenditure per student (dollars)", ylab = "Graduation Rate")
```

The above scatterplots explore some of the factors which might be related to student graduation rates. From the upper-left plot, it appears there is a weak positive relationship between the number of faculty with PhDs and graduation rates. The upper-right plot appears to indicate that there isn't relationship between the number of faculty with terminal degrees and graduation rates. The bottom-left plot indicates that as student-faculty ratios increase, graduation rates generally tend to decrease. Lastly, the bottom-right plot seems to show that there is a definite positive relationship between instructional expenditure per student and graduation rates, with higher expenditures corresponding to higher graduation rates.

# Applied Exercise 2.4.9

**This exercise involves the `Auto` data set studied in the lab. Make sure that the missing values have been removed from the data.**
```{r}
Auto = na.omit(Auto)
dim(Auto)
```
## 2.4.9.a
**Which of the predictors are quantitative, and which are qualitative?**
```{r}
head(Auto)
```
The quantitative variables are `mpg`, `displacement`, `horsepower`, `weight`, and `acceleration`. Depending on the context, we may want to treat `cylinders` and `year` as quantitative predictors or qualitative ones. Lastly, `origin` and `name` are qualitative predictors. `origin` is a quantitative encoding of a car's country of origin, where 1 being American, 2 being European, and 3 being Japanese.

## 2.4.9.b
**What is the *range* of each quantitative predictor? You can answer this using the `range()` function.**
```{r}
?range
```
```{r}
range(Auto$mpg)
```
```{r}
range(Auto$cylinders)
```
```{r}
range(Auto$displacement)
```
```{r}
range(Auto$horsepower)
```
```{r}
range(Auto$weight)
```
```{r}
range(Auto$acceleration)
```
```{r}
range(Auto$year)
```
We have the following ranges for each quantitative predictor:

- `mpg` = 37.6
- `cylinders` = 5
- `displacement` = 387
- `horsepower` = 184
- `weight` = 3527
- `acceleration` = 16.8
- `year` = 12

## 2.4.9.c
**What is the mean and standard deviation of each quantitative predictor?**
```{r}
colMeans(Auto[, 1:7])
```
```{r}
apply(Auto[, 1:7], MARGIN = 2, FUN = "sd")
```

We have the following mean and standard deviation for each quantitative predictor:

- `mpg`: mean = 23.45, standard deviation = 7.81
- `cylinders`: mean = 5.47, standard deviation = 1.71
- `displacement`: mean = 194.41, standard deviation = 104.64
- `horsepower`: mean = 104.47, standard deviation = 38.49
- `weight`: mean = 2977.58, standard deviation = 849.40
- `acceleration`: mean = 15.54, standard deviation = 2.76
- `year`: mean = 75.98, standard deviation = 3.68

## 2.4.9.d
**Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?**

```{r}
apply(Auto[-(10:85), 1:7], MARGIN = 2, FUN = "range")
```
```{r}
apply(Auto[-(10:85), 1:7], MARGIN = 2, FUN = "mean")
```
```{r}
apply(Auto[-(10:85), 1:7], MARGIN = 2, FUN = "sd")
```

We have the following range, mean,standard deviation for each quantitative predictor after the 10th through 85th rows have been removed:

- `mpg`: range = 35.6, mean = 24.40, standard deviation = 7.87
- `cylinders`: range = 5, mean = 5.37, standard deviation = 1.65
- `displacement`: range = 387, mean = 187.24, standard deviation = 99.68
- `horsepower`: range = 184, mean = 100.72, standard deviation = 35.71
- `weight`: range = 3348, mean = 2935.97, standard deviation = 811.30
- `acceleration`: range = 16.3, mean = 15.73, standard deviation = 2.69
- `year`: mean = 77.15, standard deviation = 3.11

## 2.4.9.e
**Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.**

```{r}
par(mfrow = c(2, 2))
plot(Auto$displacement, Auto$mpg, xlab = "Engine displacement (cubic inches)", ylab = "Miles per gallon")
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "Miles per gallon")
plot(Auto$weight, Auto$mpg, xlab = "Car weight (pounds)", ylab = "Miles per gallon")
plot(Auto$year, Auto$mpg, xlab = "Model Year", ylab = "Miles per gallon")
```

See discussion in Part 6 below.

```{r}
par(mfrow = c(2, 2))
plot(Auto$year, Auto$acceleration, xlab = "Model Year", ylab = "0 to 60mph time (seconds)")
plot(Auto$year, Auto$displacement, xlab = "Model Year", ylab = "Engine displacement (cubic inches)")
plot(Auto$year, Auto$weight, xlab = "Model Year", ylab = "Car weight (pounds)")
plot(Auto$year, Auto$horsepower, xlab = "Model Year", ylab = "Horsepower")
```

Looking at how various car characteristics change with model year, we see that there aren't any strong relationships. There are still some weak relationships, such as max engine displacement, car weight, and horsepower generally decreasing from 1970 to 1982. From a historical perspective, these changes could be in response to the 1973 and 1979 oil crises, in which spikes in oil prices pushed auto manufacturers to take measures to improve the efficiency of their cars.

```{r}
par(mfrow = c(2, 2))
plot(Auto$weight, Auto$acceleration, xlab = "Car weight (pounds)", ylab = "0 to 60mph time (seconds)")
plot(Auto$cylinders, Auto$acceleration, xlab = "Number of engine cylinders", ylab = "0 to 60mph time (seconds)")
plot(Auto$displacement, Auto$acceleration, xlab = "Engine displacement (cubic inches)", ylab = "0 to 60mph time (seconds)")
plot(Auto$horsepower, Auto$acceleration, xlab = "Horsepower", ylab = "0 to 60mph time (seconds)")
```

Next, I explored the relationship between the number of seconds it takes a car to accelerate from 0 to 60 miles per hour and a number of different factors. As expected, the 0-to-60 time clearly decreases with increased engine displacement and increased horsepower. There is also a weak relationship that as the number of engine cylinders increases the 0-to-60 time tends to decrease. While it may seem counter-intuitive at first, the 0-to-60 time also tends to decrease with car weight. This makes more sense in the context of the two scatterplots below, which shows that the higher weight is correlated with higher horsepower and higher engine displacement.

```{r}
par(mfrow = c(1, 1))
plot(Auto$weight, Auto$horsepower, xlab = "Car weight (pounds)", ylab = "Horsepower")

par(mfrow = c(1,1))
plot(Auto$weight, Auto$displacement, xlab = "Car weight (pounds)", ylab = "Engine displacement (cubic inches)")
```

## 2.4.9.f
**Suppose we wish to predict gas mileage (`mpg`) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting `mpg`? Justify your answer.**

Based on the scatter plots I made in part 5 which relate miles per gallon to the predictors engine displacement, horsepower, car weight, and model year, it seems as if the first three factors would be most helpful in predicting `mpg`, with model year still potentially being helpful but less so. There are clear relationships that increasing engine displacement/horsepower/car weight results in decreased fuel efficiency. There is also a weak relationship that fuel efficiency generally increased going from 1970 to 1982.

```{r}
Auto$origin[Auto$origin == 1] = "American"
Auto$origin[Auto$origin == 2] = "European"
Auto$origin[Auto$origin == 3] = "Japanese"
Auto$origin = as.factor(Auto$origin)
```

```{r}
plot(Auto$origin, Auto$mpg, xlab = "Country of origin", ylab = "Miles per gallon")
```

Looking at the above box plot, we can also see that there is a relationship between a car's country of origin and fuel efficiency, where on average Japanese cars are the most efficient, followed by European cars and then by American cars.


# Applied Exercise 2.4.10

**This exercise involves the `Boston` housing data set.**

## 2.4.10.a
**To begin, load the `Boston` data set. The `Boston` data set is part of the `MASS` *library* in `R`.**


library(MASS)
```{r}
library(MASS)
```

```{r}
head(Boston)
```

**Read about the data set:**

```{r}
?Boston
```

**How many rows are in this data set? How many columns? What do the rows and columns represent?**
```{r}
dim(Boston)
```
There are 506 observations and 14 features.

- `crim` - per capita crime rate by town.
- `zn` - proportion of residential land zoned for lots over 25,000 sq.ft.
- `indus` - proportion of non-retail business acres per town.
- `chas` - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- `nox` - nitrogen oxides concentration (parts per 10 million).
- `rm` - average number of rooms per dwelling.
- `age` - proportion of owner-occupied units built prior to 1940.
- `dis` - weighted mean of distances to five Boston employment centres.
- `rad` - index of accessibility to radial highways.
- `tax` - full-value property-tax rate per \$10,000.
- `ptratio` - pupil-teacher ratio by town.
- `black` - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- `lstat` - lower status of the population (percent).
- `medv` - median value of owner-occupied homes in \$1000s.

## 2.4.10.b
**Make some pairwise scatterplots of the predictors (columns) in the data set. Describe your findings.**

```{r}
par(mfrow = c(2, 2))
plot(Boston$age, Boston$medv, xlab = "Percent of units built prior to 1940", ylab = "Median home value in $1000s")
plot(Boston$lstat, Boston$medv, xlab = "Percent of lower status residents", ylab = "Median home value in $1000s")
plot(Boston$medv, Boston$ptratio, xlab = "Median home value in $1000s", ylab = "Pupil-teacher ratio")
plot(as.factor(Boston$chas), Boston$medv, xlab = "Borders Charles River", ylab = "Median home value in $1000s")
```

First, I generated some plots to explore the relationship between median home value and a number of non-crime factors. There aren't any especially clear patterns I can discern from thes plots aside from the expected result that as a tracts with higher median home values have a greater proportion of lower-status residence. Also, it appears as if tracts that border the Charles river are a high a slightly higher median home value on average.

```{r}
par(mfrow = c(2, 2))
plot(Boston$medv, Boston$nox, xlab = "Median home value in $1000s", ylab = "Nitric oxides concentration (parts per 10 million)")
plot(Boston$indus, Boston$nox, xlab = "Percent of non-retail business acres", ylab = "Nitric oxides concentration (parts per 10 million)")
plot(Boston$medv, Boston$black, xlab = "Median home value in $1000s", ylab = "1000(Proportion of black residents - 0.63)^2")
plot(Boston$dis, Boston$medv, xlab = "Weighted distance to Boston employment centers", ylab = "Median home value in $1000s")
```

The first two scatter plots in this next group explore factors that might relate to the concentration of nitric oxides. While there isn't a strong relationship, it appears that tracts with higher median home value also weakly tend to have lower concentrations of nitric oxides. There is a much clearer relationship with the percentage of non-retail business acres -- tracts with a higher proportion of non-retail business acres tend to have higher concentrations of nitric oxides. The bottom two plots look at some more factors which might be related to the median home value of a tract. 

The bottom-left plot seems to indicate that there is a relationship between the value of `black` and `medv`, where `black` increases as `medv` increases. If I am interpreting this correctly, this means that tracts with high median home values have a very low (close to 0%) proportion of Black residents, while tracts with low median home values have a much higher proportion (close to 63%). The bottom-right plot appears to indicate that there is also a relationship between proximity to Boston employment centers and median home value, with home values generally increasing as one gets further away from the employment centers.

## 2.4.10.c
**Are any of the predictors associated with per capita crime rate? If so, explain the relationship.**

```{r}
par(mfrow = c(2, 2))
plot(Boston$black, Boston$crim, xlab = "1000(Proportion of black residents - 0.63)^2", ylab = "Per capita crime rate")
plot(Boston$lstat, Boston$crim, xlab = "Percent of lower status residents", ylab = "Per capita crime rate")
plot(Boston$medv, Boston$crim, xlab = "Median home value in $1000s", ylab = "Per capita crime rate")
plot(Boston$dis, Boston$crim, xlab = "Weighted distance to Boston employment centers", ylab = "Per capita crime rate")
```

Based on the above four scatter plots, it appears that there are pretty clear relationships between crime rate and median home value, percent of lower status residents, and proximity to Boston employment centers. Tracts with lower home values tend to have higher crime rates, as do tracts which are closer to Boston employment centers. In addiion, tracts with higher proportion of lower status residents tend to have higher crime rates. I was also curious if there would be a relationship between crime rate and `black`, which serves as some kind of measurement for the proportion of Black residents. Based on the scatter plot between those two variables, there doesn't appear to be a clear relationship.

## 2.4.10.d
**Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor**

```{r}
par(mfrow = c(2, 2))
hist(Boston$crim, xlab = "Per capita crime rate", main = "Histogram of Boston crime rates")
hist(Boston$tax, xlab = "Tax rate per 10000 USD", main = "Histogram of Boston tax rates")
hist(Boston$ptratio, xlab = "Pupil-teacher ratio", main = "Histogram of Boston pupil-teacher ratios")
```
```{r}
summary(Boston[, c(1, 10, 11)])
```

Based on the histograms and the numerical summary, there do appear to be tracts within Boston which have particularly high crime rates, tax rates, or pupil-teacher ratios. The minimum crime rate is 0.00632, while the maximum is 88.97620, with a median of 0.25651. The minimum tax rate is \\$187 per \\$10000, while the maximum is \\$711, with a median of \\$330. The minimum pupil-teacher ratio is 12.60 pupils per teacher, while the maximum is 22, with a median of 19.05. Given the median value, the maximum pupil-teacher ratio in the data set isn't outrageously high, since about half of the tracts have a ratio of 19 or more.

## 2.4.10.e
**How many of the census tracts in this data set bound the Charles river?**

```{r}
sum(Boston$chas)
```

In this data set, 35 tracts neighbor the Charles river.

## 2.4.10.f
**What is the median pupil-teacher ratio among towns in this data set?**

```{r}
summary(Boston$ptratio)
```

The median pupil-teacher ratio among towns in this data set is 19.05 pupils per teacher.

## 2.4.10.g
**Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.**

```{r}
t(subset(Boston, medv == min(Boston$medv)))
```

Two of the tracts of South Boston have the lowest median value of owner-occupied homes, at $5000. Both of these tracts have very high crime rates compared to the overall range for that variable, with values 38.3518 and 67.9208 putting them far into the upper quartile and into the range of being outliers. These tracts have no land zoned for residential lots of 25000 sq. ft., though this is in line with at least half of the tracts in the overall set given the median for `ZN` is 0. The two tracts do have a relatively high proportion of non-retail business acres, with values of 18.1 being right at the third quartile. Similarly, the tracts also have concentrations of nitric oxides in the upper quartile of the overall set with a value of 0.693 parts per ten million. The average number of rooms per dwelling for these two tracts is at the low end, with values of 5.453 and 5.683 putting them at the bottom quartile. Next, these two tracts are among those with the highest proportion of owner-occupied homes built prior to 1940, with a value of 100. The tracts are also quite close Boston employment centers with `DIS` values of 1.4896 and 1.4254 putting them at the bottom quartile. The tracts also are very close to radial highways with the maximum value of `RAD` at 24. Next, the tracts have above average property tax rates, with a value of \\$666 per \\$10000, putting them at the third quartile. The pupil-teacher ratio of 20.2 also puts these tracts at the third quartile. The tracts have relatively high values for `B`, though one tract has a maximum value while the other, with a value of 384.97, is in between the first and second quartiles. Lastly, the tracts have a high proportion of lower status residents (values of 30.59 and 22.98), putting them in the top quartile of the data.

<p>
In summary, these two tracts with the lowest median value of owner-occupied homes have predictors generally at the extreme ends of their respective ranges.
</p>
## 2.4.10.h 
**In this data set, how many of the census tracts  average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.**

```{r}
dim(subset(Boston, rm > 7))
```

```{r}
dim(subset(Boston, rm > 8))
```
```{r}
summary(subset(Boston, rm > 8))
```
From the numerical summary, one thing that stands out is that the tracts which average at least eight rooms per dwelling have low crime rates, low concentrations of nitric oxides, low proportions of Black residents (high values of `black`), and low proportions of lower status residents compared to the overall data set.

# Applied Exercise 3.7.8

**This question involves the use of simple linear regression on the `Auto` data set.**
```{r}
library(ISLR)
library(MASS)
Auto = na.omit(Auto)
```

## 3.7.8.a
**Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summary()` function to print the results. Comment on the output. For example:**

### 3.7.8.a.i
**Is there a relationship between the predictor and the response.**

### 3.7.8.a.ii
**How strong is the relationship between the predictor and the response?**
### 3.7.8.a.iii
**Is the relationship between the predictor and the response positive or negative.**
### 3.7.8.a.iV
**What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95% confidence and prediction intervals?**

```{r}
auto.lin.fit = lm(mpg ~ horsepower, data = Auto)
summary(auto.lin.fit)
```
Simple linear regression gives a model $\hat{Y} = 39.935861 - 0.157845X_1$ between the predictor `horsepower` and the response `mpg`. A p-value of essentially zero for $\hat{\beta}_1 = -0.157845$ gives very strong evidence that there is a relationship between `mpg` and `horsepower`Since $R^2 = 0.6059$, approximately 60.6% of the variability in `mpg` is explained by a linear regression onto `horsepower`. This is a modest relationship between the predictor and the response, since as discussed in the chapter we can improve  our $R^2$ value to 0.688 by including a quadratic term. The value of $\hat{\beta}_1$ itself indicates that in the model each increase of 1 horsepower results on average in a decrease of 0.157845 miles per gallon. In other words, in this model there is a negative relationship between the predictor and the response. 
```{r}
predict(auto.lin.fit, data.frame(horsepower = 98), interval = "confidence")
```

```{r}
predict(auto.lin.fit, data.frame(horsepower = 98), interval = "prediction")
```

Plugging in a `horsepower` value of 98 gives a predicted `mpg` of 24.46708. The 95% confidence interval for this prediction is (23.97308, 24.96108) and the 95% prediction interval is (14.8094, 34.12467)
## 3.7.8.b
**Plot the response and the predictor. Use the `abline()` function to display the least squares regression line.**
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "Miles per gallon")
abline(auto.lin.fit, lwd = 3, col = "red")
```
## 3.7.8.c
**Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.**
```{r}
par(mfrow = c(2, 2))
plot(auto.lin.fit)
```

Looking at the Residuals vs. Fitted plot, there is a clear U-shape to the residuals, which is a strong indicator of non-linearity in the data. This, when combined with an inspection of the plot in Part 2, tells us that the simple linear regression model is not a good fit. In addition, when looking at the Residuals vs. Leverage plot, there are some high leverage points (remember that after dropping the rows with null values, there are 392 observations in the data set, giving an average leverage value of $2/392 \approx 0.0051$) which also have high standardized residual values (greater than 2), which is also of concern for the simple linear regression model. There are also a number of observations with a standardized residual value of 3 or more, which is evidence to suggest that they would be possibile outliers if we didn't already have the suspicion that the data is non-linear.

# Applied Exercise 3.7.9

**This question involves the use of multiple linear regression on the `Auto` data set.**
```{r}
Auto = na.omit(Auto)
head(Auto)
```
Note that the `origin` column actually contains categorical data, even though it is coded using integers. In order to make my life a little easier for performing regression, I'm going replace the values in that column with their meanings and convert it to a factor column. There are also [other options for coding categorical variables](https://stats.idre.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/), such as using the `factor()` function directly within `lm()`, or using the `C()` function to have more control over the contrast coding.

```{r}
Auto$origin[Auto$origin == 1] = "American"
Auto$origin[Auto$origin == 2] = "European"
Auto$origin[Auto$origin == 3] = "Japanese"
Auto$origin = as.factor(Auto$origin)
head(Auto)
```
## 3.7.9.a
**Produce a scatterplot matrix which includes all of the variables in the data set.**

```{r}
pairs(~mpg + cylinders + displacement + horsepower + weight + acceleration + year, Auto)
```

Since `origin` and `name` are categorical columns, I'm excluding them from the scatterplot matrix.

## 3.7.9.b

**Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the `name` variable, which is qualitative.**

```{r}
cor(Auto[,-c(8, 9)])
```
Since the `origin` column is also qualitative, I excluded it along with the `name` column when computing the matrix of correlations.

## 3.7.9.c
**Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Use the `summary()` function to print the results. Comment on the output. For instance:**
### 3.7.9.c.i
**Is there a relationship between the predictors and the response?**
### 3.7.9.c.ii
**Which predictors appear to have a statistically significant relationship to the response?**
### 3.7.9.c.iii
**What does the coefficient for the `year` variable suggest?**

```{r}
mpg.fit = lm(mpg ~ . - name, data = Auto)
summary(mpg.fit)
```

```{r}
contrasts(Auto$origin)
```

Since the F-statistic is 224.5, giving a p-value of essentially zero for the null hypothesis $H_0: \beta_j = 0 \text{ for all } j$, there is strong evidence to believe that there is a relationship between the predictors and the response. The predictors that appear to have a statistically significant relationship to the response `mpg` are `displacement` with a p-value of 0.001863, and `weight`, `year`, `originEuropean`, and `originJapanese` with p-values of essentially zero. The coefficients for `cylinders`, `horsepower`, and `acceleration` have p-values which are not small enough to provide evidence of a statistically significant relationship to the response `mpg`. The coefficient of 0.777 for the `year` variable suggests that when we fix the number of engine cylinders, engine displacement, horsepower, weight, acceleration, and country of origin, fuel efficiency increases on average by about 0.777 miles per gallon each year. In other words, the model suggests that we would expect cars from 1971 to be more fuel efficient by 0.777 miles per gallon on average compared to equivalent cars from 1970. Also of interest are the coefficients for `originEuropean` and `originJapanese`, which suggest that compared to equivalent cars from the United States, we would expect European cars to be more fuel efficient by 2.630 miles per gallon on average, and Japanese cars to be more fuel efficient by 2.853 miles per gallon on average. Lastly, the $R^2$ value of 0.8242 indicates that about 82% of the variation in `mpg` is explained by this least squares regression model.

#### Answer 3.7.9.c.i
Yes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis

#### Answer 3.7.9.c.ii
Looking at the p-values associated with each preditor's t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower, and acceleration do not.

#### Answer 3.7.9.c.iii
The regression coefficient for year, 0.7508 suggests that for every one year, mpg increases by the coefficient. In other words, cars become more fuel efficient every year by almost 1 mpg/year.

## 3.7.9.d

**Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?**

```{r}
par(mfrow = c(2, 2))
plot(mpg.fit)
```

The fit does not appear to be accurate because there is a discernible curve pattern to the residual plots. Form the leverage plot, point 14 appears to have high leverage, although not a high magnitude residual.

There are possible outliers as seen in the plot of studentized residuals becuase there are data with a value greater than 3.

## 3.7.9.e

**Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?**

```{r}
lm.fit2 = lm(Auto$mpg~Auto$cylinders*Auto$displacement+Auto$displacement*Auto$weight)
summary(lm.fit2)
```

From the correlation matrix, I obtained the two highest correlated pairs and used them in picking my interaction effects. From the p-values, we can see that the interaction between displacement and weight is statistically signifcant, while the interactiion between cylinders and displacement is not.

## 3.7.9.f
**Try a few different transformations of the variables, such as $\log(X), \sqrt{X}, X^2$. Comment on your findings.**

To get a sense of which transformations I want to try out for each quantitative variable, focusing on `displacement`, `horsepower` and `weight`, I'll look at the scatterplots of each one versus `mpg`.

```{r}
par(mfrow = c(2, 2))
plot(Auto$displacement, Auto$mpg)
plot(Auto$horsepower, Auto$mpg)
plot(Auto$weight, Auto$mpg)
plot(Auto$acceleration, Auto$mpg)
```

The book already explored nonlinear transformations of `horsepower` to predict `mpg`, so I will first look at transforms of `acceleration`.

```{r}
summary(lm(mpg ~ acceleration, data = Auto))
```

```{r}
par(mfrow = c(2, 2))
plot(lm(mpg ~ acceleration, data = Auto))
```

It appears that there might be heteroscedasticity, or non-constant variances in the error terms, so let's first see how applying a logarithmic transportation affects the model.

```{r}
summary(lm(mpg ~ log(acceleration), data = Auto))
```

```{r}
par(mfrow = c(2, 2))
plot(lm(mpg ~ log(acceleration), data = Auto))
```

While the transformation did bump up the $R^2$ value very slightly, it didn't really do anything to help with the residuals. This is probably due to the fact that two cars with the same 0 to 60 mile per hour time could be quite different in other ways that would affect fuel economy, such has differences in engine efficiency. For the remainder of the problem, let's turn our attention the the relationship between engine displacement and fuel efficiency. From the scatterplot, it is pretty clear that there is a nonlinear relationship between the two quantities. Let's start off by comparing a linear model to one that also includes the quadratic term.

```{r}
displacement.linear = lm(mpg ~ displacement, data = Auto)
summary(displacement.linear)
```
```{r}
displacement.quadratic = lm(mpg ~ poly(displacement, 2), data = Auto)
summary(displacement.quadratic)
```

```{r}
anova(displacement.linear, displacement.quadratic)
```

As we can see, the quadratic term has a p-value of essentially zero, which is quite statistically significant. Moreover, the inclusion of the quadratic term improves the $R^2$ value from 0.6482 in the linear model to 0.6888. This, along with the above results of using the `anova()` function to compare the two models, strongly suggests that the model which includes the quadtratic term is a better fit than the model which does not include it. To finish, lets now compare the quadratic model to a quintic one.

```{r}
displacement.quintic = lm(mpg ~ poly(displacement, 5), data = Auto)
summary(displacement.quintic)
```
  
```{r}
anova(displacement.quadratic, displacement.quintic)
```
  
First, we notice that none of the terms above order 2 (i.e. the cubic, quartic, and quintic terms) have statistically significant p-values. In addition, the adjusted $R^2$ value has dropped slightly from 0.6872 in the quadratic model to 0.6861. Lastly, p-value from the `anova()` function is 0.65, which means that there is not sufficient evidence to reject the null hypothesis that the quintic model is a better fit than the quadratic one. These three pieces of evidence suggest that including terms beyond order 2 does not improve the model.
  
# Applied Exercise 3.7.10

**This question should be answered using the `Carseats` data set.**

```{r}
library(ISLR)
head(Carseats)
```

## 3.7.10.a
**Fit a multiple regression model to predict `Sales` using `Price`, `Urban`, and `US`.**

```{r}
carseats.fit.1 = lm(Sales ~ Price + Urban + US, data = Carseats)
summary(carseats.fit.1)
```

```{r}
contrasts(Carseats$Urban)
```

```{r}
contrasts(Carseats$US)
```


## 3.7.10.b
**Provide an interpretation of each coefficient in the model. Be careful -- some of the variables in the model are qualitative!**

The coefficient of -0.054459 for `Price` means that, for a given location (i.e. fixed values of `Urban` and `US`), increasing the price of a car seat by \\$1 results in a decrease of sales by approximately 54.46 units, on average, in the model. The coefficient of -0.021916 for `UrbanYes` means that, for a given carseat price point and value of `US`, the model predicts urban areas to have approximately 22 fewer carseat sales on average compared to non-urban areas. The coefficient of 1.200573 for `USYes` means that, for a given carseat price point and value of `Urban`, the model predicts that stores in the United States have 1201 more carseat sales on average than stores outside the United States.

## 3.7.10.c
**Write out the model in equation form, being careful to handle the qualitative variables properly.**

The model has the following equation.

\begin{equation}
    \hat{Y} = 13.043 - 0.054X_1 - 0.022X_2 + 1.200X_3
\end{equation}

Here, $\hat{y}$ is the estimated carseat sales, in thousands of car seats; $x_{1j}$ is the price of the carseat at the jth store, in dollars; and $x_{2j}$ and $x_{3j}$ are dummy variables to represent whether or not the $j$th store at is located in an urban area and in the United States, respectively. More concretely, $x_{2j}$ and $x_{3j}$ use the following coding scheme.

\begin{align}
    x_{2j} &= 
    \begin{cases}
        1, & \text{if the $j$th store is in an urban location} \\
        0, & \text{if the $j$th store is not in an urban location}
    \end{cases}\\
    x_{3j} &= 
    \begin{cases}
        1, & \text{if the $j$th store is in the United States} \\
        0, & \text{if the $j$th store is not in the United States}
    \end{cases}
\end{align}

## 3.7.10.d
**For which of the predictors can you reject the null hypothesis $H_0 : \beta_j = 0$?**

The p-values for the intercept, `Price`, and `USYes` are all essentially zero, which provides strong evidence to reject the null hypothesis $H_0: \beta_j = 0$ for those predictors. The p-value for `UrbanYes`, however, is 0.936, so there is no evidence to reject the null hypothesis that it has a non-zero coefficient in the true relationship between the predictors and `Sales`.

## 3.7.10.e
**On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.**

```{r}
carseats.fit.2 = lm(Sales ~ Price + US, data = Carseats)
summary(carseats.fit.2)
```

## 3.7.10.f
**How well do the models in (a) and (e) fit the data?**

```{r}
par(mfrow = c(2, 2))
plot(carseats.fit.1)
```

```{r}
par(mfrow = c(2, 2))
plot(carseats.fit.2)
```

The models in Part 1 and Part 5 both fit the data about equally well, with identical $R^2$ values of 0.2393. In addition, when comparing the diagnostic plots between the two models, there isn't any discernable visual differences that would strongly indicate that one model is a better fit than the other.


## 3.7.10.g
**Using the model from (e), obtain 95% confidence intervals for the coefficient(s).**

```{r}
confint(carseats.fit.2)
```

## 3.7.10.h
**Is there evidence of outliers or high leverage observations in the model from Part 5?**

When we look at the residuals vs. leverage plot for the model from Part 5 that I generated in Part 6, we see that there are a number of observations with standardized residuals close to 3 in absolute value. Those observations are possible outliers. We can also see in the same plot that there are number of high leverage points with leverage values greatly exceeding the average leverage of $3/400 = 0.0075$, though those high leverage observations are not likely outliers, as they have studentized residual values with absolute value less than 2.
